{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('D:/Data Science/Symbiosis/Dataset/diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(columns=[\"Outcome\"])\n",
    "y=df[\"Outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest= train_test_split(x,y,test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bagging Meta Estimator/Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "model= BaggingClassifier(n_estimators=10)\n",
    "model.fit(xtrain,ytrain)\n",
    "ypred=model.predict(xtest)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BaggingClassifier in module sklearn.ensemble._bagging:\n",
      "\n",
      "class BaggingClassifier(sklearn.base.ClassifierMixin, BaseBagging)\n",
      " |  BaggingClassifier(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      " |  \n",
      " |  A Bagging classifier.\n",
      " |  \n",
      " |  A Bagging classifier is an ensemble meta-estimator that fits base\n",
      " |  classifiers each on random subsets of the original dataset and then\n",
      " |  aggregate their individual predictions (either by voting or by averaging)\n",
      " |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      " |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      " |  tree), by introducing randomization into its construction procedure and\n",
      " |  then making an ensemble out of it.\n",
      " |  \n",
      " |  This algorithm encompasses several works from the literature. When random\n",
      " |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      " |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      " |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      " |  of the dataset are drawn as random subsets of the features, then the method\n",
      " |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      " |  on subsets of both samples and features, then the method is known as\n",
      " |  Random Patches [4]_.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <bagging>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.15\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  base_estimator : object, default=None\n",
      " |      The base estimator to fit on random subsets of the dataset.\n",
      " |      If None, then the base estimator is a\n",
      " |      :class:`~sklearn.tree.DecisionTreeClassifier`.\n",
      " |  \n",
      " |  n_estimators : int, default=10\n",
      " |      The number of base estimators in the ensemble.\n",
      " |  \n",
      " |  max_samples : int or float, default=1.0\n",
      " |      The number of samples to draw from X to train each base estimator (with\n",
      " |      replacement by default, see `bootstrap` for more details).\n",
      " |  \n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples.\n",
      " |  \n",
      " |  max_features : int or float, default=1.0\n",
      " |      The number of features to draw from X to train each base estimator (\n",
      " |      without replacement by default, see `bootstrap_features` for more\n",
      " |      details).\n",
      " |  \n",
      " |      - If int, then draw `max_features` features.\n",
      " |      - If float, then draw `max_features * X.shape[1]` features.\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether samples are drawn with replacement. If False, sampling\n",
      " |      without replacement is performed.\n",
      " |  \n",
      " |  bootstrap_features : bool, default=False\n",
      " |      Whether features are drawn with replacement.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization error.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit\n",
      " |      a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* constructor parameter.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel for both :meth:`fit` and\n",
      " |      :meth:`predict`. ``None`` means 1 unless in a\n",
      " |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      " |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random resampling of the original dataset\n",
      " |      (sample wise and feature wise).\n",
      " |      If the base estimator accepts a `random_state` attribute, a different\n",
      " |      seed is generated for each instance in the ensemble.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : estimator\n",
      " |      The base estimator from which the ensemble is grown.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when :meth:`fit` is performed.\n",
      " |  \n",
      " |  estimators_ : list of estimators\n",
      " |      The collection of fitted base estimators.\n",
      " |  \n",
      " |  estimators_samples_ : list of arrays\n",
      " |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      " |      estimator. Each subset is defined by an array of the indices selected.\n",
      " |  \n",
      " |  estimators_features_ : list of arrays\n",
      " |      The subset of drawn features for each base estimator.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.svm import SVC\n",
      " |  >>> from sklearn.ensemble import BaggingClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=100, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = BaggingClassifier(base_estimator=SVC(),\n",
      " |  ...                         n_estimators=10, random_state=0).fit(X, y)\n",
      " |  >>> clf.predict([[0, 0, 0, 0]])\n",
      " |  array([1])\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      " |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      " |  \n",
      " |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      " |         1996.\n",
      " |  \n",
      " |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      " |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      " |         1998.\n",
      " |  \n",
      " |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      " |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BaggingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseBagging\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Average of the decision functions of the base classifiers.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples. The columns correspond\n",
      " |          to the classes in sorted order, as they appear in the attribute\n",
      " |          ``classes_``. Regression and binary classification are special\n",
      " |          cases with ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the class with\n",
      " |      the highest mean predicted probability. If base estimators do not\n",
      " |      implement a ``predict_proba`` method, then it resorts to voting.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the base\n",
      " |      estimators in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the mean predicted class probabilities of the base estimators in the\n",
      " |      ensemble. If base estimators do not implement a ``predict_proba``\n",
      " |      method, then it resorts to voting and the predicted class probabilities\n",
      " |      of an input sample represents the proportion of estimators predicting\n",
      " |      each class.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseBagging:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a Bagging ensemble of estimators from the training\n",
      " |         set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted.\n",
      " |          Note that this is supported only if the base estimator supports\n",
      " |          sample weighting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseBagging:\n",
      " |  \n",
      " |  estimators_samples_\n",
      " |      The subset of drawn samples for each base estimator.\n",
      " |      \n",
      " |      Returns a dynamically generated list of indices identifying\n",
      " |      the samples used for fitting each member of the ensemble, i.e.,\n",
      " |      the in-bag samples.\n",
      " |      \n",
      " |      Note: the list is re-created at each call to the property in order\n",
      " |      to reduce the object memory footprint by not storing the sampling\n",
      " |      data. Thus fetching the property may be slower than expected.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (BaggingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8051948051948052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86       107\n",
      "           1       0.70      0.64      0.67        47\n",
      "\n",
      "    accuracy                           0.81       154\n",
      "   macro avg       0.77      0.76      0.76       154\n",
      "weighted avg       0.80      0.81      0.80       154\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "acc=accuracy_score(ytest,ypred)\n",
    "print(\"Accuracy is:\",acc)\n",
    "cm=confusion_matrix(ytest,ypred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model=RandomForestClassifier()\n",
    "model.fit(xtrain,ytrain)\n",
    "ypred=model.predict(xtest)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7857142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85       107\n",
      "           1       0.65      0.64      0.65        47\n",
      "\n",
      "    accuracy                           0.79       154\n",
      "   macro avg       0.75      0.74      0.75       154\n",
      "weighted avg       0.78      0.79      0.79       154\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "acc=accuracy_score(ytest,ypred)\n",
    "print(\"Accuracy is:\",acc)\n",
    "cm=confusion_matrix(ytest,ypred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `round(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 1.0 (renaming of 0.25).\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization accuracy.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0, 1)`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7530936012691698\n",
      "{'max_features': 'sqrt', 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "#tune hyperparameter\n",
    "#parameter\n",
    "max_features=[\"sqrt\", \"log2\"]\n",
    "n_estimators=[10,100,1000]\n",
    "#parameter Grid\n",
    "grid={\"max_features\":max_features,\"n_estimators\":n_estimators}\n",
    "\n",
    "#CV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "cv=RepeatedStratifiedKFold(n_splits=10,n_repeats=3)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_cv=GridSearchCV(estimator=model,param_grid=grid,cv=cv,scoring=\"accuracy\")\n",
    "res=grid_cv.fit(xtrain,ytrain)\n",
    "print(res.best_score_)\n",
    "print(res.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RandomForestClassifier(max_features='sqrt',n_estimators=1000)\n",
    "model.fit(xtrain,ytrain)\n",
    "ypred=model.predict(xtest)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8181818181818182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87       107\n",
      "           1       0.71      0.68      0.70        47\n",
      "\n",
      "    accuracy                           0.82       154\n",
      "   macro avg       0.79      0.78      0.78       154\n",
      "weighted avg       0.82      0.82      0.82       154\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "acc=accuracy_score(ytest,ypred)\n",
    "print(\"Accuracy is:\",acc)\n",
    "cm=confusion_matrix(ytest,ypred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
